# -*- coding: utf-8 -*-
"""Email_Campaign.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lBxhXL7TE-fg3Kkzx0DmX5jC0E3R7Z-R
"""

# importing all relevant python libraries
import warnings
warnings.filterwarnings("ignore")

import math

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import RandomizedSearchCV

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.metrics import confusion_matrix

import pickle

# define a function to find the number and percentage of missing values in a dataframe
def get_missing_values_count_and_percentage(dataframe):
  num = 0
  for column in dataframe.columns:
    count = dataframe[column].isnull().sum()
    percentage = count/dataframe.shape[0]*100
    if percentage > 0:
      num += 1
      print(f"{column}: {count}({round(percentage, 2)})%")
  if num == 0:
    print("No missing values in the dataframe")

# define a function to calculate the vif of all features
# returns a dataframe
def calculate_vif_of_all_features(dataframe):
  temp_df = pd.DataFrame()
  temp_df['Feature'] = dataframe.columns
  temp_df['VIF'] = [variance_inflation_factor(dataframe.values, column_index) for column_index in range(dataframe.shape[1])]

  return temp_df

# define a function to calculate the upper and lower outlier boundary
# returns a tuple (upper_outlier_bound, lower_outlier_bound)
def get_outlier_boundaries(dataframe, column):
  percentile_25 = np.nanpercentile(dataframe[column],25)
  percentile_75 = np.nanpercentile(dataframe[column],75)
  iqr = (percentile_75 - percentile_25)
  upper_outlier_bound = percentile_75 + 1.5*iqr
  lower_outlier_bound = percentile_25 - 1.5*iqr

  return (upper_outlier_bound, lower_outlier_bound)

# define a function to calculate the number and percentage of outliers in a column
# returns a tuple (count, percentage)
def get_outlier_count_and_percentage(dataframe, column):
  # print(dataframe.shape[0])
  upper_outlier_bound, lower_outlier_bound = get_outlier_boundaries(dataframe, column)

  count = 0
  for value in dataframe[column]:
    if value > upper_outlier_bound or value < lower_outlier_bound:
      count += 1
  percentage = round(count/dataframe.shape[0]*100, 2)

  return (count, percentage)

from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             roc_auc_score, roc_curve, confusion_matrix)
import matplotlib.pyplot as plt
import seaborn as sns

def calculate_binary_model_metrics(trained_model, X_train, y_train, X_test, y_test):

    print("The best parameters: ")
    for key, value in trained_model.best_params_.items():
        print(f"{key}={value}")
    print(f"\nBest score: {trained_model.best_score_}\n")

    y_train_pred = trained_model.predict(X_train)
    y_test_pred = trained_model.predict(X_test)

    y_train_proba = trained_model.predict_proba(X_train)[:, 1]
    y_test_proba = trained_model.predict_proba(X_test)[:, 1]

    metrics_dict = {}

    metrics_dict['Train_Accuracy'] = accuracy_score(y_train, y_train_pred)
    metrics_dict['Test_Accuracy'] = accuracy_score(y_test, y_test_pred)
    metrics_dict['Train_Precision'] = precision_score(y_train, y_train_pred)
    metrics_dict['Test_Precision'] = precision_score(y_test, y_test_pred)
    metrics_dict['Train_Recall'] = recall_score(y_train, y_train_pred)
    metrics_dict['Test_Recall'] = recall_score(y_test, y_test_pred)
    metrics_dict['Train_F1_Score'] = f1_score(y_train, y_train_pred)
    metrics_dict['Test_F1_Score'] = f1_score(y_test, y_test_pred)
    metrics_dict['Train_ROC_AUC'] = roc_auc_score(y_train, y_train_proba)
    metrics_dict['Test_ROC_AUC'] = roc_auc_score(y_test, y_test_proba)

    print(f"--- Training Data ---")
    print(f"Accuracy : {metrics_dict['Train_Accuracy']:.4f}")
    print(f"Precision: {metrics_dict['Train_Precision']:.4f}")
    print(f"Recall   : {metrics_dict['Train_Recall']:.4f}")
    print(f"F1 Score : {metrics_dict['Train_F1_Score']:.4f}")
    print(f"ROC AUC  : {metrics_dict['Train_ROC_AUC']:.4f}\n")

    print(f"--- Testing Data ---")
    print(f"Accuracy : {metrics_dict['Test_Accuracy']:.4f}")
    print(f"Precision: {metrics_dict['Test_Precision']:.4f}")
    print(f"Recall   : {metrics_dict['Test_Recall']:.4f}")
    print(f"F1 Score : {metrics_dict['Test_F1_Score']:.4f}")
    print(f"ROC AUC  : {metrics_dict['Test_ROC_AUC']:.4f}\n")

    fpr, tpr, _ = roc_curve(y_test, y_test_proba)
    plt.figure(figsize=(10, 6))
    plt.plot(fpr, tpr, label=f"Test ROC AUC = {metrics_dict['Test_ROC_AUC']:.4f}")
    plt.plot([0, 1], [0, 1], 'k--')
    plt.title("ROC Curve")
    plt.ylabel("True Positive Rate")
    plt.xlabel("False Positive Rate")
    plt.legend(loc='lower right')
    plt.show()

    cm = confusion_matrix(y_test, y_test_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    return metrics_dict

# defining a function to generate a count plot
def generate_count_plot(data):
  """
  Function to generate a count plot
  """
  plt.rcParams["figure.figsize"] = [7.50, 3.50]
  plt.rcParams["figure.autolayout"] = True
  fig, ax = plt.subplots()
  data.value_counts().plot(ax=ax, kind='bar')
  plt.show()

# define a function to generate horizontal box plot
# prints a box plot
def generate_horizontal_box_plot(dataframe, x_feature, y_feature=None):
  sns.set_theme(rc={'figure.figsize': (8,4)},style='whitegrid',palette='muted')
  if y_feature != None:
    ax = sns.boxplot(x=dataframe[x_feature], y=dataframe[y_feature])
  else:
    ax = sns.boxplot(x=dataframe[x_feature], y=None)
  ax.grid(False)

# define a function to generate vertical box plot
# prints a box plot
def generate_vertical_box_plot(dataframe, y_feature, x_feature=None):
  sns.set_theme(rc={'figure.figsize': (4,8)},style='whitegrid',palette='muted')
  if x_feature != None:
    ax = sns.boxplot(y=dataframe[y_feature], x=dataframe[x_feature])
  else:
    ax = sns.boxplot(y=dataframe[y_feature], x=None)
  ax.grid(False)

# get count of each unique value from a particular column
# and output its as a dataframe
def get_count_from_column(df, column_label):
  df_grpd = df[column_label].value_counts()
  df_grpd = pd.DataFrame({'index':df_grpd.index, 'count':df_grpd.values})
  return df_grpd

# add value to the top of each bar
def add_value_label(x_list,y_list):
    for i in range(1, len(x_list)+1):
        plt.text(i-1,y_list[i-1],y_list[i-1], ha="center", fontweight='bold')

# plot bar graph from grouped data
def plot_bar_graph_from_column(df, column_label):
  df_grpd = get_count_from_column(df, column_label)

  df_grpd.plot(x='index', y='count', kind='bar', figsize=(3*df_grpd.shape[0], 6))
  add_value_label(df_grpd['index'].tolist(), df_grpd['count'].tolist())
  plt.xlabel(column_label)
  plt.ylabel("Count")
  plt.xticks(rotation='horizontal')
  plt.gca().legend_.remove()
  plt.show()

# generate density plots of a feature in a dataframe
# prints a density plot
def density_plot_of_a_feature(dataframe, feature):
  plt.figure(figsize=(8, 6))
  sns.distplot(dataframe[feature])
  plt.show()

# generate density plots of all features in a dataframe
# prints a density plots
def density_plot_of_all_features(dataframe):
  columns = dataframe.describe().columns.tolist()

  columns_num = 3
  rows_num = math.ceil(len(columns)/columns_num)
  fig, axes = plt.subplots(rows_num, columns_num, figsize=(10*columns_num, 8*rows_num))

  row = -1
  column = columns_num - 1
  for feature in columns:
    if column == (columns_num - 1):
      row += 1
      column = 0
    else:
      column += 1
    sns.distplot(ax=axes[row, column], a=dataframe[feature])
    axes[row, column].set_title(f"{feature} Distribution")

  if len(columns) % columns_num != 0:
    for column_num in range(column + 1, columns_num):
      fig.delaxes(axes[rows_num - 1][column_num])

# define a function to plot a stacked bar to show percentage of a feature in a grouped parameter
# so that it can be used in the later stages also

# get the count of unique values in secondary column
# segmented by each unique value in primary column
def get_count_of_unique_values(df, pri_column_label, sec_column_label):
  # finding unique values in secondary column for grouping
  values = sorted([x for x in df[sec_column_label].unique() if str(x) != 'nan'])

  # creating a list of dataframes that gives the value of each unique value in primary column
  # a dataframe is created for each unique value in secondary column
  list_of_counts_df = [df[df[sec_column_label] == value].groupby(pri_column_label)[sec_column_label].count().reset_index(name=f'{value}')
                       for value in values]

  # merge all dataframes into one dataframe
  df_merged = list_of_counts_df[0]
  for i in range(1, len(list_of_counts_df)):
    df_merged = pd.merge(df_merged, list_of_counts_df[i], how='inner', on=pri_column_label)

  return df_merged

# plotting a stacked bar graph to represent the count of unique values in secondary column
# segmented by each unique value in primary column
def stacked_bar_graph_with_count(df, pri_column_label, sec_column_label):
   # computing the percentage of unique values in secondary column contributed by each unique value in primary column
  df_merged = get_count_of_unique_values(df, pri_column_label, sec_column_label)

  ax = df_merged.plot(x=pri_column_label, kind='bar', stacked=True, figsize=(12,6))
  for bar in ax.patches:
    height = bar.get_height()
    width = bar.get_width()
    x = bar.get_x()
    y = bar.get_y()
    label_text = str(height)
    label_x = x + width / 2
    label_y = y + height / 2
    ax.text(label_x, label_y, label_text, ha='center', va='center', fontweight='bold')
  plt.xticks(rotation='horizontal')
  plt.title(f"Percentage of different categories of {sec_column_label} across each {pri_column_label} category")
  plt.show()

# get the % of unique values in secondary column
# segmented by each unique value in primary column
def get_percentage_of_unique_values(df, pri_column_label, sec_column_label):
  # finding unique values in secondary column for grouping
  values = [x for x in df[sec_column_label].unique() if str(x) != 'nan']

  # creating a dataframe that gives the count of each unique value in primary column
  df_merged = get_count_of_unique_values(df, pri_column_label, sec_column_label)

  # computing the percentage of unique values in secondary column contributed by each unique value in primary column
  df_merged['total_count'] = df_merged.sum(axis=1, numeric_only=True)
  for value in values:
    df_merged[f'{value}'] = round(df_merged[f'{value}'] / df_merged['total_count'] * 100)
  df_merged.drop('total_count', axis=1, inplace=True)

  return df_merged

# plotting a stacked bar graph to represent the % of unique values in secondary column
# segmented by each unique value in primary column
def stacked_bar_graph_with_percentage(df, pri_column_label, sec_column_label):
   # computing the percentage of unique values in secondary column contributed by each unique value in primary column
  df_merged = get_percentage_of_unique_values(df, pri_column_label, sec_column_label)

  ax = df_merged.plot(x=pri_column_label, kind='bar', stacked=True, figsize=(12,6))
  for bar in ax.patches:
    height = bar.get_height()
    width = bar.get_width()
    x = bar.get_x()
    y = bar.get_y()
    label_text = str(height) + " %"
    label_x = x + width / 2
    label_y = y + height / 2
    ax.text(label_x, label_y, label_text, ha='center', va='center', fontweight='bold')
  plt.xticks(rotation='horizontal')
  plt.title(f"Percentage of different categories of {sec_column_label} across each {pri_column_label} category")
  plt.show()

# define a function to plot bar graph with three features
# prints a bar graph
def plot_bar_graph_with_three_features(dataframe, x_feature, y_feature, z_feature, y_label):
  plt.figure(figsize=(26, 6))

  X = dataframe[x_feature].tolist()
  Y = dataframe[y_feature].tolist()
  Z = dataframe[z_feature].tolist()

  X_axis_length = np.arange(len(X))

  plt.bar(X_axis_length - 0.2, Y, 0.4, label = y_feature)
  plt.bar(X_axis_length + 0.2, Z, 0.4, label = z_feature)

  min_limit = 0.9 * min(dataframe[y_feature].min(), dataframe[z_feature].min())
  max_limit = 1.1 * max(dataframe[y_feature].max(), dataframe[z_feature].max())
  plt.ylim(min_limit, max_limit)

  plt.xticks(X_axis_length, X)
  plt.xlabel(x_feature)
  plt.ylabel(y_label)
  plt.legend()
  plt.show()

"""# Reading Data"""

opened_df=pd.read_csv("/content/email_opened_table.csv")
opened_df.head()

campaign_df=pd.read_csv("/content/email_table.csv")
clicked_df=pd.read_csv("/content/link_clicked_table.csv")

campaign_df['opened'] = campaign_df['email_id'].isin(opened_df['email_id']).astype(int)
campaign_df['clicked'] = campaign_df['email_id'].isin(clicked_df['email_id']).astype(int)



"""# Data Inspection

The given data contains information regarding the mails sent as a part of Gmail-based e-mail marketing campaign. Lets first examine the data present in it.
"""

# exploring the head of the dataframe
campaign_df.head()

# exploring the tail of the dataframe
campaign_df.tail()

# looking into a brief summary of dataframe
campaign_df.describe()

# total number of rows in the dataset
num_of_rows = campaign_df.shape[0]
print(f"Total no. of rows: {num_of_rows}")

# number of duplicate rows
num_of_dup_rows = campaign_df[campaign_df.duplicated()].shape[0]
print(f"No. of duplicate rows: {num_of_dup_rows}")

"""*   The dataframe contains 100000 rows of data and has zero duplicate rows."""

# exploring the columns of the dataframe
campaign_df.info()

"""*   The dataframe contains 9 columns.
*   no columns have missing values.
*   Three columns require conversion of datatypes.
"""

# unique values in each column of the dataframe
print(campaign_df.apply(lambda col: col.unique()))

# finding the number and percentage of missing values in the data
get_missing_values_count_and_percentage(campaign_df)

# create a density plot to show the distribution of values in user_past_purchases

density_plot_of_a_feature(campaign_df, 'user_past_purchases')

campaign_df

# generate box plots to show the distribution of user_past_purchases in each category of Email_Type and email_version

fig, axes = plt.subplots(1, 2, figsize=(20, 6))
sns.boxplot(ax=axes[0], data=campaign_df, x='user_past_purchases', y='email_version', orient='h')
sns.boxplot(ax=axes[1], data=campaign_df, x='user_past_purchases', y='email_text', orient='h')

"""Lets check its box plot to check for outliers."""

# create a box plot to show the distribution of values in user_past_purchases

generate_horizontal_box_plot(campaign_df, 'user_past_purchases')
outlier_count, outlier_percentage = get_outlier_count_and_percentage(campaign_df, 'user_past_purchases')
print(f"Number of outliers: {outlier_count}")
print(f"Percentage of outliers: {outlier_percentage}%")

density_plot_of_a_feature(campaign_df, 'user_past_purchases')

# create a density plot to show the distribution of values in hours

density_plot_of_a_feature(campaign_df, 'hour')

# number and percentage of missing values in the dataframe
get_missing_values_count_and_percentage(campaign_df)

"""### Conversion of Column Datatype"""

# datatypes of columns in the dataframe
campaign_df.dtypes

campaign_df['email_id'].duplicated().sum()

# datatypes of columns in the dataframe
campaign_df.dtypes

#i assumed that  first you have to open the email then clicked on link it so those emails which are clicked by users also opened first


# # 0 = ignored, 1 = opened, 2 = clicked
# def engagement_level(row):
#     if row['clicked'] == 1:
#         return 2
#     elif row['opened'] == 1:
#         return 1
#     else:
#         return 0

# campaign_df['engagement'] =campaign_df.apply(engagement_level, axis=1)
# campaign_df=campaign_df.drop(['opened','clicked'],axis=1)

# plot bar graph to show the count of each category in engagement

plot_bar_graph_from_column(campaign_df, 'clicked')

# plot the distribution of all features
density_plot_of_all_features(campaign_df)

campaign_df

# plot stacked bar graphs to show the percentage of e-mails in each feature category for every engagement
categorical_features = ['email_text', 'email_version', 'user_country','weekday']
for feature in categorical_features:
  stacked_bar_graph_with_percentage(campaign_df, 'opened', feature)

for feature in categorical_features:
  stacked_bar_graph_with_percentage(campaign_df, 'clicked', feature)

#  Calculate the correlation matrix
# Select only numerical features for correlation calculation
numerical_features = campaign_df.select_dtypes(include=np.number).columns
corr_matrix = campaign_df[numerical_features].corr()

# Plot the heatmap
plt.figure(figsize=(12, 10))  # Adjust the figure size as needed
sns.heatmap(corr_matrix, annot=True, cmap='viridis', fmt=".2f")
plt.title("Correlation Matrix of All Features")
plt.show()

# generate box plots to show the distribution of numerical features in each category of engagement
numerical_features = ['user_past_purchases','hour']

fig, axes = plt.subplots(5, 1, figsize=(18, 36))

for row, feature in enumerate(numerical_features):
  sns.boxplot(ax=axes[row], data=campaign_df, x=feature, y='clicked', orient='h')
  axes[row].set_title(f"Distribution of {feature} across each engagement category")

fig, axes = plt.subplots(1, 2, figsize=(18, 6))

sns.kdeplot(data=campaign_df, x='hour', hue='clicked', fill=True, ax=axes[0])
axes[0].set_title('Density of Email Send Hour by Clicked vs. Not Clicked')

sns.kdeplot(data=campaign_df, x='user_past_purchases', hue='clicked', fill=True, ax=axes[1])
axes[1].set_title('Density of Past Purchases by Clicked vs. Not Clicked')
axes[1].set_xlim(0, 25) # Adjust limit if needed

plt.tight_layout()
plt.show()

# --- Create Plots ---
fig, axes = plt.subplots(1, 2, figsize=(18, 6))

sns.violinplot(data=campaign_df, x='clicked', y='hour', ax=axes[0])
axes[0].set_title('Distribution of Send Hour by Engagement')

sns.violinplot(data=campaign_df, x='clicked', y='user_past_purchases', ax=axes[1])
axes[1].set_title('Distribution of Past Purchases by Engagement')

plt.tight_layout()
plt.show()

# --- Bin the user_past_purchases feature ---
bins = [-1, 0, 3, 7, 12, campaign_df['user_past_purchases'].max()]
labels = ['0', '1-3', '4-7', '8-12', '13+']
campaign_df['purchase_bins'] = pd.cut(campaign_df['user_past_purchases'], bins=bins, labels=labels)

# --- Create Plots ---
fig, axes = plt.subplots(2, 1, figsize=(15, 12))

sns.barplot(data=campaign_df, x='hour', y='clicked', ax=axes[0])
axes[0].set_title('Click-Through Rate by Hour of the Day')

sns.barplot(data=campaign_df, x='purchase_bins', y='clicked', ax=axes[1])
axes[1].set_title('Click-Through Rate by Number of Past Purchases')

plt.tight_layout()
plt.show()

# exploring the head of the dataframe
campaign_df.head()

campaign_df.drop('email_id', axis=1, inplace=True)

# exploring the head of the dataframe
campaign_df.head()

# exploring the head of the dataframe
campaign_df.head()

"""Now, all features have VIF below 5."""

# generate a box plot for user_past_purchases
generate_horizontal_box_plot(campaign_df, 'user_past_purchases')

# count and percentage of outliers in user_past_purchases
count, perc = get_outlier_count_and_percentage(campaign_df, 'user_past_purchases')
print(f"Outliers in user_past_purchases : {count} ({perc}%)")

# remove outliers
upper_boundary, lower_boundary = get_outlier_boundaries(campaign_df, 'user_past_purchases')
campaign_df = campaign_df[(campaign_df['user_past_purchases'] > lower_boundary) & (campaign_df['user_past_purchases'] < (upper_boundary-0.2))]

# numerical features
numerical_features = ['user_past_purchases', 'hour']

# generate density plot for numerical features
for feature in numerical_features:
  plt.figure(figsize=(9, 6))
  sns.distplot(campaign_df[feature]).set(title=f'{feature} Distribution')
  plt.show()

from sklearn.preprocessing import OneHotEncoder

categorical_features = ['email_text', 'email_version']

ohe = OneHotEncoder(dtype=int, handle_unknown='ignore', sparse_output=False) # remove 'sparse' argument and set sparse_output to False
# ohe.set_output(transform='dense') #to specify output format as 'dense' , remove this line causing the error
ohe.fit(campaign_df[categorical_features])
encoded_features = list(ohe.get_feature_names_out(categorical_features))
campaign_df[encoded_features] = ohe.transform(campaign_df[categorical_features])
campaign_df.drop(categorical_features, axis=1, inplace=True)

# exploring the head of the dataframe
campaign_df.head()

from sklearn.preprocessing import OneHotEncoder

# Assuming your dataframe is named 'campaign_df'
categorical_features = ['weekday']

# Create a OneHotEncoder object
ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore', dtype=int)  # sparse=False for dense output

# Fit the encoder to the 'weekday' column
ohe.fit(campaign_df[categorical_features])

# Get the feature names after encoding
encoded_features = list(ohe.get_feature_names_out(categorical_features))

# Transform the 'weekday' column and add the encoded features to the dataframe
campaign_df[encoded_features] = ohe.transform(campaign_df[categorical_features])

# Drop the original 'weekday' column
campaign_df.drop(categorical_features, axis=1, inplace=True)

from sklearn.preprocessing import OneHotEncoder

# Assuming your dataframe is named 'campaign_df'
categorical_features = ['user_country']

# Create a OneHotEncoder object
ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore', dtype=int)  # sparse=False for dense output

# Fit the encoder to the 'weekday' column
ohe.fit(campaign_df[categorical_features])

# Get the feature names after encoding
encoded_features = list(ohe.get_feature_names_out(categorical_features))

# Transform the 'weekday' column and add the encoded features to the dataframe
campaign_df[encoded_features] = ohe.transform(campaign_df[categorical_features])

# Drop the original 'weekday' column
campaign_df.drop(categorical_features, axis=1, inplace=True)

# exploring the head of the dataframe
campaign_df.head()

# Assuming your dataframe is named 'campaign_df'
# Select only numerical features for correlation calculation
numerical_features = campaign_df.select_dtypes(include=np.number).columns
corr_matrix = campaign_df[numerical_features].corr()
corr_matrix

# Plot the heatmap
plt.figure(figsize=(12, 10))  # Adjust the figure size as needed
sns.heatmap(corr_matrix, annot=True, cmap='viridis', fmt=".2f")
plt.title("Correlation Matrix of Numerical Features")
plt.show()

"""## Train-Test Split

First, we will separate independant features and dependant feature.
"""

# independant features
X = campaign_df.drop('clicked', axis=1)

# dependant feature
y = campaign_df['clicked']

"""Now, lets split them into training and test data with 25% test data."""

# split the datasets to training and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 0)
print(X_train.shape)
print(X_test.shape)

"""## Feature Scaling

Scaling of features is important because algorithms like logistic regression, which is based on gradient descent, performs better when features are relatively on a similar scale.

So training and test data of independant features are scaled using standardization.
"""

# standardization of independant training and testing data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train.drop('purchase_bins', axis=1))
X_test = scaler.transform(X_test.drop('purchase_bins', axis=1))

campaign_df = pd.read_csv('email_table.csv')
opened_df = pd.read_csv('email_opened_table.csv')
clicked_df = pd.read_csv('link_clicked_table.csv')
campaign_df['opened'] = campaign_df['email_id'].isin(opened_df['email_id']).astype(int)
campaign_df['clicked'] = campaign_df['email_id'].isin(clicked_df['email_id']).astype(int)

# --- Data Preparation (as before) ---
categorical_features = ['email_text', 'email_version', 'weekday', 'user_country']
model_data = pd.get_dummies(campaign_df, columns=categorical_features, drop_first=True)
X = model_data.drop(columns=['email_id', 'opened', 'clicked'])
y = model_data['clicked']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
scaler = StandardScaler()
numerical_features = ['hour', 'user_past_purchases']
X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])
X_test[numerical_features] = scaler.transform(X_test[numerical_features])

X_test

y

"""## Handling Data Imbalance

We have already established in EDA that the dataset is highly imbalanced. If we train the models without fixing this problem, the model will be completely biased. So, we will use 2 different techniques to balance training data: Random Undersampling and SMOTE.
"""

# use undersampling to eliminate data imbalance
rus = RandomUnderSampler(replacement=True)
X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)

# use oversampling to eliminate data imbalance
smote = SMOTE()
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# generating a count plot to check data imbalance
generate_count_plot(y_train)
generate_count_plot(y_train_rus)
generate_count_plot(y_train_smote)

"""Now we have a balance dataset. Each class in undersampled data has number of samples equal to the smallest original class whereas each class in oversampled data has number of samples equal to the largest original class.

Lets create a dataframe to store all calculated model metrics.
"""

# create a dataframe to store metrics related to models
metrics_table = pd.DataFrame(columns=['Model', 'Sampling', 'Train_Accuracy', 'Test_Accuracy', 'Train_Precision', 'Test_Precision',
                                      'Train_Recall', 'Test_Recall', 'Train_F1Score', 'Test_F1Score', 'Train_ROC_AUC', 'Test_ROC_AUC'])

"""## Model Training

### Logistic Regression with Hyperparameter Tuning
"""

# initialize hyperparameters for logistic regression
log_reg = LogisticRegression(multi_class='multinomial', class_weight='balanced')
parameters = {'solver':['lbfgs', 'newton-cg', 'saga'],
              'C':[0.01, 0.1, 1],
              'max_iter':[50, 80, 100]}

y

# train data with logistic regression on random undersampling
log_reg_rus = RandomizedSearchCV(log_reg, parameters, cv=5, n_iter=10)
log_reg_rus.fit(X_train_rus, y_train_rus)

# train data with logistic regression on SMOTE
log_reg_smote = RandomizedSearchCV(log_reg, parameters, cv=5, n_iter=10)
log_reg_smote.fit(X_train_smote, y_train_smote)

# train data with decision tree on random undersampling

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# --- Use the best model found by the search ---
best_model = log_reg_rus.best_estimator_

# --- Evaluate on the Training Set ---
y_train_pred = best_model.predict(X_train_rus)
y_train_proba = best_model.predict_proba(X_train_rus)[:, 1]

train_accuracy = accuracy_score(y_train_rus, y_train_pred)
train_precision = precision_score(y_train_rus, y_train_pred)
train_recall = recall_score(y_train_rus, y_train_pred)
train_f1 = f1_score(y_train_rus, y_train_pred)
train_roc_auc = roc_auc_score(y_train_rus, y_train_proba)

# --- Evaluate on the Test Set ---
y_test_pred = best_model.predict(X_test)
y_test_proba = best_model.predict_proba(X_test)[:, 1]

test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred)
test_recall = recall_score(y_test, y_test_pred)
test_f1 = f1_score(y_test, y_test_pred)
test_roc_auc = roc_auc_score(y_test, y_test_proba)

# --- Add metrics to the metrics table ---
metrics_table.loc[len(metrics_table.index)] = ['Logistic Regression', 'RandomUnderSampling',
                                             train_accuracy, train_precision, train_recall,
                                             train_f1, train_roc_auc, test_accuracy,
                                             test_precision, test_recall, test_f1,
                                             test_roc_auc]

print("Metrics successfully calculated and added to the table.")
print(metrics_table.tail(1))



from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

best_model_smote = log_reg_smote.best_estimator_

y_train_pred_smote = best_model_smote.predict(X_train_smote)
y_train_proba_smote = best_model_smote.predict_proba(X_train_smote)[:, 1]

train_accuracy = accuracy_score(y_train_smote, y_train_pred_smote)
train_precision = precision_score(y_train_smote, y_train_pred_smote)
train_recall = recall_score(y_train_smote, y_train_pred_smote)
train_f1 = f1_score(y_train_smote, y_train_pred_smote)
train_roc_auc = roc_auc_score(y_train_smote, y_train_proba_smote)

y_test_pred_smote = best_model_smote.predict(X_test)
y_test_proba_smote = best_model_smote.predict_proba(X_test)[:, 1]

test_accuracy = accuracy_score(y_test, y_test_pred_smote)
test_precision = precision_score(y_test, y_test_pred_smote)
test_recall = recall_score(y_test, y_test_pred_smote)
test_f1 = f1_score(y_test, y_test_pred_smote)
test_roc_auc = roc_auc_score(y_test, y_test_proba_smote)

metrics_table.loc[len(metrics_table.index)] = ['Logistic Regression', 'SMOTE',
                                             train_accuracy, train_precision, train_recall,
                                             train_f1, train_roc_auc, test_accuracy,
                                             test_precision, test_recall, test_f1,
                                             test_roc_auc]

print("Metrics for SMOTE model calculated and added to the table.")
print(metrics_table.tail(1))

"""### Decision Tree with Hyperparameter Tuning"""

# initialize hyperparameters for decision tree classifier
decision_tree = DecisionTreeClassifier()
parameters = {'max_depth': [5, 10, None],
              'min_samples_leaf': [1, 2, 5],
              'min_samples_split': [2, 5, 10],
              'max_leaf_nodes': [5, 20, 100],
              'max_features': ['auto', 'sqrt', 'log2']}

dt_rus = RandomizedSearchCV(decision_tree, parameters, cv=5, n_iter=10)
dt_rus.fit(X_train_rus, y_train_rus)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

best_model_dt = dt_rus.best_estimator_

y_train_pred_dt = best_model_dt.predict(X_train_rus)
y_train_proba_dt = best_model_dt.predict_proba(X_train_rus)[:, 1]

train_accuracy = accuracy_score(y_train_rus, y_train_pred_dt)
train_precision = precision_score(y_train_rus, y_train_pred_dt)
train_recall = recall_score(y_train_rus, y_train_pred_dt)
train_f1 = f1_score(y_train_rus, y_train_pred_dt)
train_roc_auc = roc_auc_score(y_train_rus, y_train_proba_dt)

y_test_pred_dt = best_model_dt.predict(X_test)
y_test_proba_dt = best_model_dt.predict_proba(X_test)[:, 1]

test_accuracy = accuracy_score(y_test, y_test_pred_dt)
test_precision = precision_score(y_test, y_test_pred_dt)
test_recall = recall_score(y_test, y_test_pred_dt)
test_f1 = f1_score(y_test, y_test_pred_dt)
test_roc_auc = roc_auc_score(y_test, y_test_proba_dt)

metrics_table.loc[len(metrics_table.index)] = ['Decision Tree', 'RandomUnderSampling',
                                             train_accuracy, train_precision, train_recall,
                                             train_f1, train_roc_auc, test_accuracy,
                                             test_precision, test_recall, test_f1,
                                             test_roc_auc]

print("Metrics for Decision Tree (RUS) model calculated and added to the table.")
print(metrics_table.tail(1))

# train data with logistic regression on SMOTE
dt_smote = RandomizedSearchCV(decision_tree, parameters, cv=5, n_iter=10)
dt_smote.fit(X_train_smote, y_train_smote)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

best_model_smote = dt_smote.best_estimator_

y_train_pred_smote = best_model_smote.predict(X_train_smote)
y_train_proba_smote = best_model_smote.predict_proba(X_train_smote)[:, 1]

train_accuracy = accuracy_score(y_train_smote, y_train_pred_smote)
train_precision = precision_score(y_train_smote, y_train_pred_smote)
train_recall = recall_score(y_train_smote, y_train_pred_smote)
train_f1 = f1_score(y_train_smote, y_train_pred_smote)
train_roc_auc = roc_auc_score(y_train_smote, y_train_proba_smote)

y_test_pred_smote = best_model_smote.predict(X_test)
y_test_proba_smote = best_model_smote.predict_proba(X_test)[:, 1]

test_accuracy = accuracy_score(y_test, y_test_pred_smote)
test_precision = precision_score(y_test, y_test_pred_smote)
test_recall = recall_score(y_test, y_test_pred_smote)
test_f1 = f1_score(y_test, y_test_pred_smote)
test_roc_auc = roc_auc_score(y_test, y_test_proba_smote)

metrics_table.loc[len(metrics_table.index)] = ['Logistic Regression', 'SMOTE',
                                             train_accuracy, train_precision, train_recall,
                                             train_f1, train_roc_auc, test_accuracy,
                                             test_precision, test_recall, test_f1,
                                             test_roc_auc]

print("Metrics for SMOTE model calculated and added to the table.")
print(metrics_table.tail(1))

"""### Random Forest with Hyperparameter Tuning"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# --- Train and Evaluate Random Forest on RandomUnderSampling Data ---
print("Training Random Forest on RUS data...")
rf_rus = RandomizedSearchCV(RandomForestClassifier(random_state=42), parameters, cv=5, n_iter=10, random_state=42)
rf_rus.fit(X_train_rus, y_train_rus)

best_model_rf_rus = rf_rus.best_estimator_
y_train_pred_rf_rus = best_model_rf_rus.predict(X_train_rus)
y_train_proba_rf_rus = best_model_rf_rus.predict_proba(X_train_rus)[:, 1]
y_test_pred_rf_rus = best_model_rf_rus.predict(X_test)
y_test_proba_rf_rus = best_model_rf_rus.predict_proba(X_test)[:, 1]

metrics_table.loc[len(metrics_table.index)] = ['Random Forest', 'RandomUnderSampling',
                                             accuracy_score(y_train_rus, y_train_pred_rf_rus),
                                             precision_score(y_train_rus, y_train_pred_rf_rus),
                                             recall_score(y_train_rus, y_train_pred_rf_rus),
                                             f1_score(y_train_rus, y_train_pred_rf_rus),
                                             roc_auc_score(y_train_rus, y_train_proba_rf_rus),
                                             accuracy_score(y_test, y_test_pred_rf_rus),
                                             precision_score(y_test, y_test_pred_rf_rus),
                                             recall_score(y_test, y_test_pred_rf_rus),
                                             f1_score(y_test, y_test_pred_rf_rus),
                                             roc_auc_score(y_test, y_test_proba_rf_rus)]
print("Metrics for RF (RUS) added.")

# --- Train and Evaluate Random Forest on SMOTE Data ---
print("\nTraining Random Forest on SMOTE data...")
rf_smote = RandomizedSearchCV(RandomForestClassifier(random_state=42), parameters, cv=5, n_iter=10, random_state=42)
rf_smote.fit(X_train_smote, y_train_smote)

best_model_rf_smote = rf_smote.best_estimator_
y_train_pred_rf_smote = best_model_rf_smote.predict(X_train_smote)
y_train_proba_rf_smote = best_model_rf_smote.predict_proba(X_train_smote)[:, 1]
y_test_pred_rf_smote = best_model_rf_smote.predict(X_test)
y_test_proba_rf_smote = best_model_rf_smote.predict_proba(X_test)[:, 1]

metrics_table.loc[len(metrics_table.index)] = ['Random Forest', 'SMOTE',
                                             accuracy_score(y_train_smote, y_train_pred_rf_smote),
                                             precision_score(y_train_smote, y_train_pred_rf_smote),
                                             recall_score(y_train_smote, y_train_pred_rf_smote),
                                             f1_score(y_train_smote, y_train_pred_rf_smote),
                                             roc_auc_score(y_train_smote, y_train_proba_rf_smote),
                                             accuracy_score(y_test, y_test_pred_rf_smote),
                                             precision_score(y_test, y_test_pred_rf_smote),
                                             recall_score(y_test, y_test_pred_rf_smote),
                                             f1_score(y_test, y_test_pred_rf_smote),
                                             roc_auc_score(y_test, y_test_proba_rf_smote)]
print("Metrics for RF (SMOTE) added.")

# --- Display Final Metrics Table ---
print("\n--- Final Model Comparison ---")
print(metrics_table)

"""### XGBoost with Hyperparameter Tuning"""

import xgboost as xgb
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# --- Train and Evaluate XGBoost on RandomUnderSampling Data ---
print("Training XGBoost on RUS data...")
xgb_rus = RandomizedSearchCV(xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42),
                             parameters, cv=5, n_iter=10, random_state=42)
xgb_rus.fit(X_train_rus, y_train_rus)

best_model_xgb_rus = xgb_rus.best_estimator_
y_train_pred_xgb_rus = best_model_xgb_rus.predict(X_train_rus)
y_train_proba_xgb_rus = best_model_xgb_rus.predict_proba(X_train_rus)[:, 1]
y_test_pred_xgb_rus = best_model_xgb_rus.predict(X_test)
y_test_proba_xgb_rus = best_model_xgb_rus.predict_proba(X_test)[:, 1]

metrics_table.loc[len(metrics_table.index)] = ['XGBoost', 'RandomUnderSampling',
                                             accuracy_score(y_train_rus, y_train_pred_xgb_rus),
                                             precision_score(y_train_rus, y_train_pred_xgb_rus),
                                             recall_score(y_train_rus, y_train_pred_xgb_rus),
                                             f1_score(y_train_rus, y_train_pred_xgb_rus),
                                             roc_auc_score(y_train_rus, y_train_proba_xgb_rus),
                                             accuracy_score(y_test, y_test_pred_xgb_rus),
                                             precision_score(y_test, y_test_pred_xgb_rus),
                                             recall_score(y_test, y_test_pred_xgb_rus),
                                             f1_score(y_test, y_test_pred_xgb_rus),
                                             roc_auc_score(y_test, y_test_proba_xgb_rus)]
print("Metrics for XGBoost (RUS) added.")


# --- Train and Evaluate XGBoost on SMOTE Data ---
print("\nTraining XGBoost on SMOTE data...")
xgb_smote = RandomizedSearchCV(xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42),
                               parameters, cv=5, n_iter=10, random_state=42)
xgb_smote.fit(X_train_smote, y_train_smote)

best_model_xgb_smote = xgb_smote.best_estimator_
y_train_pred_xgb_smote = best_model_xgb_smote.predict(X_train_smote)
y_train_proba_xgb_smote = best_model_xgb_smote.predict_proba(X_train_smote)[:, 1]
y_test_pred_xgb_smote = best_model_xgb_smote.predict(X_test)
y_test_proba_xgb_smote = best_model_xgb_smote.predict_proba(X_test)[:, 1]

metrics_table.loc[len(metrics_table.index)] = ['XGBoost', 'SMOTE',
                                             accuracy_score(y_train_smote, y_train_pred_xgb_smote),
                                             precision_score(y_train_smote, y_train_pred_xgb_smote),
                                             recall_score(y_train_smote, y_train_pred_xgb_smote),
                                             f1_score(y_train_smote, y_train_pred_xgb_smote),
                                             roc_auc_score(y_train_smote, y_train_proba_xgb_smote),
                                             accuracy_score(y_test, y_test_pred_xgb_smote),
                                             precision_score(y_test, y_test_pred_xgb_smote),
                                             recall_score(y_test, y_test_pred_xgb_smote),
                                             f1_score(y_test, y_test_pred_xgb_smote),
                                             roc_auc_score(y_test, y_test_proba_xgb_smote)]
print("Metrics for XGBoost (SMOTE) added.")

# --- Display Final Metrics Table ---
print("\n--- Final Model Comparison ---")
print(metrics_table)

"""**LIGHTGBM And Hyperparameter Tuning**

### KNN with Hyperparameter Tuning
"""

# initialize hyperparameters for knn classifier
knn = KNeighborsClassifier()
parameters = {'n_neighbors':[5, 10, 15],
              'weights':['uniform','distance'],
              'metric':['minkowski','euclidean','manhattan'],
              'leaf_size':[10, 20, 30]}

# train data with knn on random undersampling
knn_rus = RandomizedSearchCV(knn, parameters, cv=5, n_iter=10)
knn_rus.fit(X_train_rus, y_train_rus)

# model evaluation
model_evaluation = calculate_model_metrics(knn_rus, X_train_rus, y_train_rus, X_test, y_test)

# add metrics to metrics table
metrics_table.loc[len(metrics_table.index)] = ['KNN', 'RandomUnderSampling',
                                                model_evaluation['Train_Accuracy'], model_evaluation['Train_Precision'],
                                                model_evaluation['Train_Recall'], model_evaluation['Train_F1_Score'],
                                                model_evaluation['Train_ROC_AUC'], model_evaluation['Test_Accuracy'],
                                                model_evaluation['Test_Precision'], model_evaluation['Test_Recall'],
                                                model_evaluation['Test_F1_Score'], model_evaluation['Test_ROC_AUC']]

# train data with knn on SMOTE
knn_smote = RandomizedSearchCV(knn, parameters, cv=5, n_iter=10)
knn_smote.fit(X_train_smote, y_train_smote)

# model evaluation
model_evaluation = calculate_model_metrics(knn_smote, X_train_smote, y_train_smote, X_test, y_test)

# add metrics to metrics table
metrics_table.loc[len(metrics_table.index)] = ['KNN', 'SMOTE',
                                                model_evaluation['Train_Accuracy'], model_evaluation['Train_Precision'],
                                                model_evaluation['Train_Recall'], model_evaluation['Train_F1_Score'],
                                                model_evaluation['Train_ROC_AUC'], model_evaluation['Test_Accuracy'],
                                                model_evaluation['Test_Precision'], model_evaluation['Test_Recall'],
                                                model_evaluation['Test_F1_Score'], model_evaluation['Test_ROC_AUC']]

"""## Model Comparison"""

# print metrics table
metrics_table

"""Accuracy, precision, recall or ROC-AUC cannot be used to compare the performance of models since the data is imbalanced. So F1 score should be used to compare different models and find out which one is better. Higher the F1 score, better the model."""

# plot bar graph to show F1 scores
plot_bar_graph_with_three_features(metrics_table, 'Model', 'Train_F1Score', 'Test_F1Score', 'F1 Score')

"""The model built using LightGBM algorithm with SMOTE dataset has the highest F1 score and also all other models tends to overfit. So we can choose this model."""